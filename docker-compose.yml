version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: zookeeper
    networks:
      - uniswap-v3-event-pipeline
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      # Add this to use temporary storage
      ZOOKEEPER_dataDir: /tmp/zookeeper
    # Replace volumes with tmpfs for ephemeral storage
    tmpfs:
      - /tmp/zookeeper
    healthcheck:
      test: ['CMD', 'echo', 'ruok', '|', 'nc', 'localhost', '2181']
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    networks:
      - uniswap-v3-event-pipeline
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 1
      KAFKA_LOG_CLEANUP_POLICY: "delete"
      # Add this to ensure logs are stored in a temporary location
      KAFKA_LOG_DIRS: /tmp/kafka-logs
    # Add tmpfs for ephemeral storage
    tmpfs:
      - /tmp/kafka-logs
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s

  # Database (TimescaleDB / PostgreSQL)
  database:
    image: timescale/timescaledb:latest-pg16
    container_name: timescaledb
    networks:
      - uniswap-v3-event-pipeline
    ports:
      - "5434:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-app_db}
      POSTGRES_USER: ${POSTGRES_USER:-user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 5

  ####################
  # Data Services
  ####################
  listener:
    build:
      context: . # Path to the listener service directory
      dockerfile: listener/Dockerfile
    container_name: listener
    networks:
      - uniswap-v3-event-pipeline
    depends_on:
      kafka:
        condition: service_healthy
    env_file:
      - .env
    environment:
      # Kafka configuration
      KAFKA_BROKERS: ${KAFKA_BROKERS:-kafka:9092}
      # Configuration file path
      CONFIG_FILE: ${CONFIG_FILE:-config/pool_config.json}
      ABI_DIR: ${ABI_DIR:-config/abi}
      # Metrics port
      METRICS_PORT: ${METRICS_PORT:-8001}
      # ETH_RPC_URL must be in .env
    ports:
      - "8001:8001" # Expose Prometheus metrics port
    volumes:
      - ./config:/app/config
    restart: unless-stopped

  consumer:
    build:
      context: . # Path to the consumer service directory
      dockerfile: consumer/Dockerfile
    container_name: consumer
    networks:
      - uniswap-v3-event-pipeline
    depends_on:
      database:
        condition: service_healthy
      kafka:
        condition: service_healthy
    env_file:
      - .env # Load environment variables from .env file
    environment:
      # Database configuration
      DATABASE_URL: postgresql://${POSTGRES_USER:-user}:${POSTGRES_PASSWORD:-password}@database:5432/${POSTGRES_DB:-app_db}
      # Kafka configuration
      KAFKA_BROKERS: ${KAFKA_BROKERS:-kafka:9092}
      KAFKA_CONSUMER_GROUP: ${KAFKA_CONSUMER_GROUP:-uniswap-event-consumer}
      # Batch processing configuration
      BATCH_SIZE: ${BATCH_SIZE:-100}
      BATCH_TIMEOUT: ${BATCH_TIMEOUT:-5.0}
      # Metrics
      METRICS_PORT: ${METRICS_PORT:-8002}
    ports:
      - "8002:8002" # Expose Prometheus metrics
    restart: unless-stopped


  prometheus:
    image: prom/prometheus:v2.51.2 # Use a specific version
    container_name: prometheus
    networks:
      - uniswap-v3-event-pipeline
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml # Mount config
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    restart: unless-stopped

  grafana:
    image: grafana/grafana:10.4.2
    container_name: grafana
    networks:
      - uniswap-v3-event-pipeline
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      # Add a provisioning directory for proper datasource configuration
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
    environment:
      # Remove these environment variables as they don't correctly configure datasources
      # GF_DATASOURCES_DEFAULT_NAME: Prometheus
      # GF_DATASOURCES_0_NAME: Prometheus  
      # GF_DATASOURCES_0_TYPE: prometheus
      # GF_DATASOURCES_0_URL: http://prometheus:9090
      # GF_DATASOURCES_0_ISDEFAULT: "true"
      # Keep these authentication settings
      GF_AUTH_ANONYMOUS_ENABLED: "true"
      GF_AUTH_ANONYMOUS_ORG_ROLE: "Admin"
      GF_AUTH_DISABLE_LOGIN_FORM: "true"
      # Add this to make Grafana accessible from outside
      GF_SERVER_DOMAIN: localhost
      GF_SERVER_ROOT_URL: "http://localhost:3000"
    depends_on:
      - prometheus
    restart: unless-stopped


networks:
  uniswap-v3-event-pipeline:
    driver: bridge

volumes:
  postgres_data:
  prometheus_data:
  grafana_data: